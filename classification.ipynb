{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38032bitd37838a0bbc64f33801bb6f569018b45",
   "display_name": "Python 3.8.0 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-4803a8222612>, line 28)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-4803a8222612>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    return word_freq\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "# def remove_special_characters(text):\n",
    "#     regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "#     text_returned = re.sub(regex,'',text)\n",
    "#     return text_returned\n",
    "\n",
    "# def finding_all_unique_words_and_freq(words):\n",
    "#     #word_freq = defaultdict(list)\n",
    "#     #list_number=[]\n",
    "#     #list_number.append(idx)\n",
    "#     for word in words:\n",
    "#         word_freq[word].append(idx)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "all_words = []\n",
    "dict_global = defaultdict(list)\n",
    "\n",
    "word_freq_in_doc = {}\n",
    "file_folder = 'data/*'\n",
    "\n",
    "idx = 1\n",
    "files_with_index = {}\n",
    "for file in glob.glob(file_folder):\n",
    "    print(file)\n",
    "#     with open(file,'r') as f:\n",
    "#         text = f.read()\n",
    "#         text = remove_special_characters(text)\n",
    "#         text = re.sub(re.compile('\\d'),'',text)\n",
    "#         sentences = sent_tokenize(text)\n",
    "#         words = word_tokenize(text)\n",
    "#         words = [word for word in words if len(words)>1]\n",
    "#         words = [word.lower() for word in words]\n",
    "#         words = [word for word in words if word not in Stopwords]\n",
    "#         word_freq={}\n",
    "#         print(words)\n",
    "#         #for word in words:\n",
    "#         #    word_freq[word] = word_freq.get(word,0) + 1\n",
    "#         #for word iword_freq.keys():\n",
    "        #    dict_global[word].append(idx)\n",
    "# #\n",
    "#         ##dict_global.update(finding_all_unique_words_and_freq(wods))\n",
    "#         idx+= 1\n",
    "        #print(idx)\n",
    "#---------------------dict_global has all the root words and file number containing the words------\n",
    "print(dict_global)\n",
    "#unique_words_all = set(dict_global.keys())  \n",
    "# # #print(unique_words_all)\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Node:\n",
    "#     def __init__(self ,docId, freq = None):\n",
    "#         self.freq = freq\n",
    "#         self.doc = docId\n",
    "#         self.nextval = None\n",
    "    \n",
    "# class SlinkedList:\n",
    "#     def __init__(self ,head = None):\n",
    "#         self.head = head\n",
    "\n",
    "# def  = 1\n",
    "# for word in unique_words_all:\n",
    "#     linked_list_data[word] = SlinkedList()\n",
    "#     linked_list_data[word].head = Node(1,Node)\n",
    "\n",
    "# #print(linked_list_data)\n",
    "\n",
    "# word_freq_in_doc = {}\n",
    "\n",
    "\n",
    "# for file in glob.glob(file_folder):\n",
    "#    print(file)\n",
    "#    with open(file,'r') as f:\n",
    "#        text = f.read()\n",
    "#        text = remove_special_characters(text)\n",
    "#        text = re.sub(re.compile('\\d'),'',text)\n",
    "#        sentences = sent_tokenize(text)\n",
    "#        words = word_tokenize(text)\n",
    "#        words = [word for word in words if len(words)>1]\n",
    "#        words = [word.lower() for word in words]\n",
    "#        words = [word for word in words if word not in Stopwords]\n",
    "#        word_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
    "#        for word in word_freq_in_doc.keys():\n",
    "#            linked_list = linked_list_data[word].head\n",
    "#            while linked_list.nextval is not None:\n",
    "#                linked_list = linked_list.nextval\n",
    "#            linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
    "#        idx = idx + 1\n",
    "# print(linked_list_data)finding_freq_of_word_in_doc(word,words):\n",
    "#     freq = words.count(word)\n",
    "\n",
    "\n",
    "# linked_list_data = {}\n",
    "# idx = 1\n",
    "# for word in unique_words_all:\n",
    "#     linked_list_data[word] = SlinkedList()\n",
    "#     linked_list_data[word].head = Node(1,Node)\n",
    "\n",
    "# #print(linked_list_data)\n",
    "\n",
    "# word_freq_in_doc = {}\n",
    "\n",
    "\n",
    "# for file in glob.glob(file_folder):\n",
    "#    print(file)\n",
    "#    with open(file,'r') as f:\n",
    "#        text = f.read()\n",
    "#        text = remove_special_characters(text)\n",
    "#        text = re.sub(re.compile('\\d'),'',text)\n",
    "#        sentences = sent_tokenize(text)\n",
    "#        words = word_tokenize(text)\n",
    "#        words = [word for word in words if len(words)>1]\n",
    "#        words = [word.lower() for word in words]\n",
    "#        words = [word for word in words if word not in Stopwords]\n",
    "#        word_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
    "#        for word in word_freq_in_doc.keys():\n",
    "#            linked_list = linked_list_data[word].head\n",
    "#            while linked_list.nextval is not None:\n",
    "#                linked_list = linked_list.nextval\n",
    "#            linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
    "#        idx = idx + 1\n",
    "# print(linked_list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'unique_words_all' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-48aae4a5435a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mzeroes_and_ones_of_all_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdifferent_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_words_all\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mzeroes_and_ones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mlinkedlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinked_list_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_words_all' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"virat\"\n",
    "query = word_tokenize(query)\n",
    "connecting_words = []\n",
    "cnt = 1\n",
    "different_words = []\n",
    "for word in query:\n",
    "    if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
    "        different_words.append(word.lower())\n",
    "    else:\n",
    "        connecting_words.append(word.lower())\n",
    "print(connecting_words)\n",
    "total_files = len(files_with_index)\n",
    "zeroes_and_ones = []\n",
    "zeroes_and_ones_of_all_words = []\n",
    "for word in (different_words):\n",
    "    if word.lower() in unique_words_all:\n",
    "        zeroes_and_ones = [0] * total_files\n",
    "        linkedlist = linked_list_data[word].head\n",
    "        print(word)\n",
    "        while linkedlist.nextval is not None:\n",
    "            zeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n",
    "            linkedlist = linkedlist.nextval\n",
    "        zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n",
    "    else:\n",
    "        print(word,\" not found\")\n",
    "        sys.exit()\n",
    "print(zeroes_and_ones_of_all_words)\n",
    "for word in connecting_words:\n",
    "    word_list1 = zeroes_and_ones_of_all_words[0]\n",
    "    word_list2 = zeroes_and_ones_of_all_words[1]\n",
    "    if word == \"and\":\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "    elif word == \"or\":\n",
    "        bitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "    elif word == \"not\":\n",
    "        bitwise_op = [not w1 for w1 in word_list2]\n",
    "        bitwise_op = [int(b == True) for b in bitwise_op]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\n",
    "zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "        \n",
    "files = []    \n",
    "print(zeroes_and_ones_of_all_words)\n",
    "lis = zeroes_and_ones_of_all_words[0]\n",
    "cnt = 1\n",
    "for index in lis:\n",
    "    if index == 1:\n",
    "        files.append(files_with_index[cnt])\n",
    "    cnt = cnt+1\n",
    "    \n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "understanding\n[1, 2]\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dx' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-10c3abf4597b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0marray_binary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m#print(array_binary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dx' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob, os\n",
    "from csv import DictWriter\n",
    "import os.path\n",
    "from reportlab.pdfgen import canvas\n",
    "os.chdir(\"D:\\Projects\\sem-6project\\data\")\n",
    "file_names=[]\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    file_names.append(file)\n",
    "\n",
    "os.chdir(\"D:\\Projects\\sem-6project\")\n",
    "\n",
    "file_exists = os.path.isfile(\"BVMCSV.csv\")\n",
    "csv_file = open(\"BVMCSV.csv\",'a+')\n",
    "csv_file.seek(0)\n",
    "#c_file=csv_file.readlines()\n",
    "dict_write = DictWriter(csv_file,fieldnames=['Names']+file_names)\n",
    "if not file_exists:\n",
    "    dict_write.writeheader()\n",
    "for key,value in dict_global.items():\n",
    "    # idx-1 is the no of files in data folder \n",
    "    print(key)\n",
    "    print(value)\n",
    "    array_binary=[0]*(dx-1)\n",
    "    #print(array_binary)\n",
    "    \n",
    "    for i in range(0,idx):\n",
    "        for val in value:\n",
    "            if (val-1)==i:\n",
    "                array_binary[i]=1\n",
    "    print(file_names)\n",
    "    print(array_binary)\n",
    "\n",
    "\n",
    "    row={}\n",
    "    row['Names']=key\n",
    "    for i in range(0,idx-1):\n",
    "        row[file_names[i]]=array_binary[i]\n",
    "\n",
    "    print(row)\n",
    "\n",
    "    dict_write.writerow(row)\n",
    "\n",
    "    \n",
    "    \n",
    "# print(idx-1)\n",
    "csv_file.close()\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "data\\data1.txt\n['understanding', 'quotes', 'life', 'understood', 'backwards', 'must', 'lived', 'forwardsany', 'fool', 'know', 'beautiful', 'people', 'known', 'known', 'defeat', 'known', 'suffering', 'known', 'struggle', 'known', 'loss', 'found', 'way', 'depths', 'sachin', 'tendulkar', 'born', 'april', 'bombay', 'indiaintroduced', 'cricket', 'age', 'tendulkar', 'became', 'indias', 'youngest', 'test', 'cricketerin', 'became', 'first', 'cricketer', 'score', 'centuries', 'runs', 'single', 'inning', 'test', 'play', 'kapil', 'dev', 'ram', 'lal', 'nikhanj', 'born', 'january', 'former', 'indian', 'cricketerhe', 'fast', 'bowler', 'hard', 'hitting', 'middle', 'order', 'batsman', 'regarded', 'one', 'greatest', 'allrounders', 'play', 'game', 'also', 'regarded', 'one', 'greatest', 'captains', 'history', 'cricket']\n2\ndata\\virat_kholi.txt\n['understanding', 'quotes', 'virat', 'kohli', 'soundpronunciation', 'helpinfo', 'born', 'november', 'indian', 'cricketer', 'currently', 'captains', 'india', 'national', 'team', 'righthanded', 'toporder', 'batsman', 'kohli', 'regarded', 'one', 'best', 'batsmen', 'world', 'plays', 'royal', 'challengers', 'bangalore', 'indian', 'premier', 'league', 'ipl', 'teams', 'captain', 'since', 'since', 'october', 'topranked', 'odi', 'batsman', 'world', 'currently', 'st', 'test', 'rankings', 'points', 'among', 'indian', 'batsmen', 'kohli', 'best', 'ever', 'test', 'rating', 'points', 'odi', 'rating', 'points', 'ti', 'rating', 'points', 'kohli', 'captained', 'india', 'unders', 'victory', 'world', 'cup', 'malaysia', 'months', 'later', 'made', 'odi', 'debut', 'india', 'sri', 'lanka', 'age', 'initially', 'played', 'reserve', 'batsman', 'indian', 'team', 'soon', 'established', 'regular', 'odi', 'middleorder', 'part', 'squad', 'world', 'cup', 'made', 'test', 'debut', 'shrugged', 'tag', 'odi', 'specialist', 'test', 'hundreds', 'australia', 'south', 'africa', 'reached', 'number', 'one', 'spot', 'icc', 'rankings', 'odi', 'batsmen', 'first', 'time', 'kohli', 'also', 'found', 'success', 'twenty', 'format', 'winning', 'man', 'tournament', 'twice', 'icc', 'world', 'twenty', 'kohli', 'appointed', 'vicecaptain', 'odi', 'team', 'handed', 'test', 'captaincy', 'following', 'mahendra', 'singh', 'dhonis', 'test', 'retirement', 'early', 'became', 'limitedovers', 'captain', 'well', 'dhoni', 'stepped', 'position', 'odis', 'kohli', 'second', 'highest', 'number', 'centuries', 'highest', 'number', 'centuries', 'runchases', 'world', 'holds', 'world', 'record', 'fastest', 'batsman', 'runs', 'odi', 'cricket', 'reaching', 'milestones', 'innings', 'respectively', 'kohli', 'recipient', 'many', 'awards', 'sir', 'garfield', 'sobers', 'trophy', 'icc', 'cricketer', 'year', 'icc', 'test', 'player', 'year', 'icc', 'odi', 'player', 'year', 'wisden', 'leading', 'cricketer', 'world', 'given', 'arjuna', 'award', 'padma', 'shri', 'sports', 'category', 'rajiv', 'gandhi', 'khel', 'ratna', 'highest', 'sporting', 'honour', 'india', 'kohli', 'ranked', 'one', 'worlds', 'famous', 'athletes', 'espn', 'one', 'valuable', 'athlete', 'brands', 'forbes', 'time', 'magazine', 'named', 'kohli', 'one', 'influential', 'people', 'world']\n3\ndefaultdict(<class 'list'>, {'understanding': [1, 2], 'quotes': [1, 2], 'life': [1], 'understood': [1], 'backwards': [1], 'must': [1], 'lived': [1], 'forwardsany': [1], 'fool': [1], 'know': [1], 'beautiful': [1], 'people': [1, 2], 'known': [1], 'defeat': [1], 'suffering': [1], 'struggle': [1], 'loss': [1], 'found': [1, 2], 'way': [1], 'depths': [1], 'sachin': [1], 'tendulkar': [1], 'born': [1, 2], 'april': [1], 'bombay': [1], 'indiaintroduced': [1], 'cricket': [1, 2], 'age': [1, 2], 'became': [1, 2], 'indias': [1], 'youngest': [1], 'test': [1, 2], 'cricketerin': [1], 'first': [1, 2], 'cricketer': [1, 2], 'score': [1], 'centuries': [1, 2], 'runs': [1, 2], 'single': [1], 'inning': [1], 'play': [1], 'kapil': [1], 'dev': [1], 'ram': [1], 'lal': [1], 'nikhanj': [1], 'january': [1], 'former': [1], 'indian': [1, 2], 'cricketerhe': [1], 'fast': [1], 'bowler': [1], 'hard': [1], 'hitting': [1], 'middle': [1], 'order': [1], 'batsman': [1, 2], 'regarded': [1, 2], 'one': [1, 2], 'greatest': [1], 'allrounders': [1], 'game': [1], 'also': [1, 2], 'captains': [1, 2], 'history': [1], 'virat': [2], 'kohli': [2], 'soundpronunciation': [2], 'helpinfo': [2], 'november': [2], 'currently': [2], 'india': [2], 'national': [2], 'team': [2], 'righthanded': [2], 'toporder': [2], 'best': [2], 'batsmen': [2], 'world': [2], 'plays': [2], 'royal': [2], 'challengers': [2], 'bangalore': [2], 'premier': [2], 'league': [2], 'ipl': [2], 'teams': [2], 'captain': [2], 'since': [2], 'october': [2], 'topranked': [2], 'odi': [2], 'st': [2], 'rankings': [2], 'points': [2], 'among': [2], 'ever': [2], 'rating': [2], 'ti': [2], 'captained': [2], 'unders': [2], 'victory': [2], 'cup': [2], 'malaysia': [2], 'months': [2], 'later': [2], 'made': [2], 'debut': [2], 'sri': [2], 'lanka': [2], 'initially': [2], 'played': [2], 'reserve': [2], 'soon': [2], 'established': [2], 'regular': [2], 'middleorder': [2], 'part': [2], 'squad': [2], 'shrugged': [2], 'tag': [2], 'specialist': [2], 'hundreds': [2], 'australia': [2], 'south': [2], 'africa': [2], 'reached': [2], 'number': [2], 'spot': [2], 'icc': [2], 'time': [2], 'success': [2], 'twenty': [2], 'format': [2], 'winning': [2], 'man': [2], 'tournament': [2], 'twice': [2], 'appointed': [2], 'vicecaptain': [2], 'handed': [2], 'captaincy': [2], 'following': [2], 'mahendra': [2], 'singh': [2], 'dhonis': [2], 'retirement': [2], 'early': [2], 'limitedovers': [2], 'well': [2], 'dhoni': [2], 'stepped': [2], 'position': [2], 'odis': [2], 'second': [2], 'highest': [2], 'runchases': [2], 'holds': [2], 'record': [2], 'fastest': [2], 'reaching': [2], 'milestones': [2], 'innings': [2], 'respectively': [2], 'recipient': [2], 'many': [2], 'awards': [2], 'sir': [2], 'garfield': [2], 'sobers': [2], 'trophy': [2], 'year': [2], 'player': [2], 'wisden': [2], 'leading': [2], 'given': [2], 'arjuna': [2], 'award': [2], 'padma': [2], 'shri': [2], 'sports': [2], 'category': [2], 'rajiv': [2], 'gandhi': [2], 'khel': [2], 'ratna': [2], 'sporting': [2], 'honour': [2], 'ranked': [2], 'worlds': [2], 'famous': [2], 'athletes': [2], 'espn': [2], 'valuable': [2], 'athlete': [2], 'brands': [2], 'forbes': [2], 'magazine': [2], 'named': [2], 'influential': [2]})\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex,'',text)\n",
    "    return text_returned\n",
    "\n",
    "def finding_all_unique_words_and_freq(words):\n",
    "    #word_freq = defaultdict(list)\n",
    "    #list_number=[]\n",
    "    #list_number.append(idx)\n",
    "    for word in words:\n",
    "        word_freq[word].append(idx)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "all_words = []\n",
    "dict_global = defaultdict(list)\n",
    "\n",
    "word_freq_in_doc = {}\n",
    "file_folder = 'data/*'\n",
    "\n",
    "idx = 1\n",
    "files_with_index = {}\n",
    "for file in glob.glob(file_folder):\n",
    "    print(file)\n",
    "    with open(file,'r') as f:\n",
    "        text = f.read()\n",
    "        text = remove_special_characters(text)\n",
    "        text = re.sub(re.compile('\\d'),'',text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if len(words)>1]\n",
    "        words = [word.lower() for word in words]\n",
    "        words = [word for word in words if word not in Stopwords]\n",
    "        word_freq={}\n",
    "        print(words)\n",
    "        for word in words:\n",
    "           word_freq[word] = word_freq.get(word,0) + 1\n",
    "        for word in word_freq.keys():\n",
    "           dict_global[word].append(idx)\n",
    "# #\n",
    "        ##dict_global.update(finding_all_unique_words_and_freq(words))\n",
    "        idx+= 1\n",
    "        print(idx)\n",
    "#---------------------dict_global has all the words and file number containing the words------\n",
    "print(dict_global)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['virat', 'hello', 'again', 'there', 'is', 'something', 'wrong', 'with', 'it', 'understanding', 'sachin']\n[]\nvirat\nhello\nsomething\nwrong\nunderstanding\nsachin\n['virat,0,1\\n', 'understanding,1,1\\n', 'sachin,1,0\\n']\n[[], [], []]\n[['0', '1'], ['1', '1'], ['1', '0']]\n[0, 1]\n"
    }
   ],
   "source": [
    "query = \"virat hello again there is something wrong with it understanding sachin\"\n",
    "query = list(query.split(\" \"))\n",
    "print(query)\n",
    "#print(Stopwords)\n",
    "query = [word for word in query if word not in Stopwords]\n",
    "\n",
    "#print(query)\n",
    "\n",
    "result=None\n",
    "#print(result)\n",
    "\n",
    "\n",
    "###   bitwise addditon for words bits #####\n",
    "# def bitwise_addition(bits):\n",
    "#     global result\n",
    "#     if result == None:\n",
    "#         result = [word for word in bits]\n",
    "#     else:\n",
    "#         for i in range(0,idx-1):\n",
    "#             result[i]=result[i]&bits[i]\n",
    "#     #print(result)\n",
    "\n",
    "# temp=[1,0]\n",
    "\n",
    "# bitwise_addition(temp)\n",
    "words=[]\n",
    "print(words)\n",
    "\n",
    "for word in query:\n",
    "    print(word)\n",
    "    csv_file = open(\"BVMCSV.csv\",'r')\n",
    "    csv_file.seek(0)\n",
    "    c_file=csv_file.readlines()\n",
    "    for line in c_file:\n",
    "        #print(line)\n",
    "        if word in line:\n",
    "            if \"Names\" not in line: \n",
    "                words.append(line)\n",
    "\n",
    "print(words)\n",
    "int_words=[[]]*len(words)\n",
    "print(int_words)\n",
    "for i in range(0,len(words)):\n",
    "    word_bin=[]\n",
    "    for j in range(0,len(words[i])):\n",
    "        #print(words[i][j])\n",
    "        #print(type(words[i][j]))\n",
    "        if words[i][j]==\"0\" or words[i][j]==\"1\":\n",
    "            #print(words[i][j])\n",
    "            word_bin.append(words[i][j])\n",
    "            #print(word_bin)\n",
    "    int_words[i]=word_bin\n",
    "print(int_words)\n",
    "result=[0]*(idx-1)\n",
    "\n",
    "for i in range(0,idx-1):\n",
    "    result[i]=int_words[0][i]\n",
    "#print(result)\n",
    "#int_words[2][1]=\"1\"\n",
    "for i in range(0,idx-1):\n",
    "    for j in range(0,len(int_words)):\n",
    "        result[i]=int(result[i])&int(int_words[j][i])\n",
    "        #print(int_words[j][i])\n",
    "\n",
    "print(result)\n",
    "\n",
    "#result contains the requires bits \n",
    "\n",
    "#print(idx-1)\n",
    "#------bitwise addition ------#\n",
    "\n",
    "#for i in range(0,idx-1):\n",
    "    #print(i)\n",
    "    #for j in range(0,len(int_words)):\n",
    "\n",
    "# print(words[1][0])\n",
    "# print(len(words))\n",
    "\n",
    "\n",
    "# for i in range(1,idx):\n",
    "#     for j in range(0,len(words)):\n",
    "#         print(words[j][i])\n",
    "# result=[]\n",
    "# for i in range(0,idx-1)):\n",
    "#     for j in range(0,len(int_words)):\n",
    "#         #if(int_words[j][i]==1 and result[i]==1)):\n",
    "#             #result[i]=result[i]&int_words[j][i]\n",
    "#         print(i)\n",
    "#         print(j)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}