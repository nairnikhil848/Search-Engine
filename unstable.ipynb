{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "#nltk.download('reuters')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  Thailand's trade deficit widened to 4.5\\n  billion baht in the first quarter of 1987 from 2.1 billion a\\n  year ago, the Business Economics Department said.\\n      It said Janunary/March imports rose to 65.1 billion baht\\n  from 58.7 billion. Thailand's improved business climate this\\n  year resulted in a 27 pct increase in imports of raw materials\\n  and semi-finished products.\\n      The country's oil import bill, however, fell 23 pct in the\\n  first quarter due to lower oil prices.\\n      The department said first quarter exports expanded to 60.6\\n  billion baht from 56.6 billion.\\n      Export growth was smaller than expected due to lower\\n  earnings from many key commodities including rice whose\\n  earnings declined 18 pct, maize 66 pct, sugar 45 pct, tin 26\\n  pct and canned pineapples seven pct.\\n      Products registering high export growth were jewellery up\\n  64 pct, clothing 57 pct and rubber 35 pct.\\n  \\n\\n\""
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "reuters.raw(fileids=['test/14832'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "JAPAN TO REVISE LONGTERM ENERGY DEMAND DOWNWARDS\n  The Ministry of International Trade and\n  Industry MITI will revise its longterm energy supplydemand\n  outlook by August to meet a forecast downtrend in Japanese\n  energy demand ministry officials said\n      MITI is expected to lower the projection for primary energy\n  supplies in the year 2000 to 550 mln kilolitres kl from 600\n  mln they said\n      The decision follows the emergence of structural changes in\n  Japanese industry following the rise in the value of the yen\n  and a decline in domestic electric power demand\n      MITI is planning to work out a revised energy supplydemand\n  outlook through deliberations of committee meetings of the\n  Agency of Natural Resources and Energy the officials said\n      They said MITI will also review the breakdown of energy\n  supply sources including oil nuclear coal and natural gas\n      Nuclear energy provided the bulk of Japans electric power\n  in the fiscal year ended March 31 supplying an estimated 27\n  pct on a kilowatthour basis followed by oil 23 pct and\n  liquefied natural gas 21 pct they noted\n  \n\n\n"
    }
   ],
   "source": [
    "# remove punctuation from all DOCs \n",
    "punctuations = set(string.punctuation)\n",
    "docslist = []\n",
    "count=0\n",
    "for index, i in  enumerate(reuters.fileids()):\n",
    "    text = reuters.raw(fileids=[i])\n",
    "    text = ''.join(ch for ch in text if ch not in punctuations)\n",
    "    docslist.append(text)\n",
    "    \n",
    "print(docslist[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize words in all DOCS\n",
    "tokenized_doc = []\n",
    "\n",
    "for doc in docslist:\n",
    "    tokentext = word_tokenize(doc)\n",
    "    tokenized_doc.append(tokentext)\n",
    "    \n",
    "print(tokenized_doc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(reuters.fileids())):\n",
    "    lowers = [word.lower() for word in tokenized_doc[x]]\n",
    "    tokenized_doc[x] = lowers\n",
    "\n",
    "tokenized_doc[2][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Creating inverse index which gives document number for each document and where word appears\n",
    "\n",
    "#first we need to create a list of all words \n",
    "\n",
    "l = tokenized_doc\n",
    "allwords = [item for sublist in l for item in sublist]\n",
    "#print(allwords[0:100])\n",
    "wordsunique = set(allwords)\n",
    "wordsunique = list(wordsunique)\n",
    "wordsunique[0:10]\n",
    "\n",
    "def unique_words():\n",
    "    l = tokenized_doc\n",
    "    allwords = [item for sublist in l for item in sublist]\n",
    "    #print(allwords[0:100])\n",
    "    wordsunique = set(allwords)\n",
    "    wordsunique = list(wordsunique)\n",
    "    return wordsunique\n",
    "    #wordsunique[0:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating functions for TD-IDF / BM25\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Creating dictonary of words\n",
    "import re\n",
    "import numpy as np\n",
    "plottest = tokenized_doc[0:1000]\n",
    "\n",
    "worddic = {}\n",
    "\n",
    "for doc in plottest:\n",
    "    for word in wordsunique:\n",
    "        if word in doc:\n",
    "            word = str(word)\n",
    "            index = plottest.index(doc)\n",
    "            positions = list(np.where(np.array(plottest[index]) == word)[0])\n",
    "            idfs = tfidf(word,doc,plottest)\n",
    "            try:\n",
    "                worddic[word].append([index,positions,idfs])\n",
    "            except:\n",
    "                worddic[word] = []\n",
    "                worddic[word].append([index,positions,idfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(plottest[0])==\"japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"worddic_1000\",\"rb\")\n",
    "worddic = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[0, [14, 47, 249, 361, 378, 411, 435, 548, 600], 0.033386270829022735],\n [2, [0], 0.015288639114382281],\n [14,\n  [0, 17, 41, 69, 88, 106, 167, 178, 216, 278, 285, 316, 444],\n  0.07362772883992666],\n [27, [0, 13, 108], 0.040278191372408144],\n [28, [74], 0.026716510775637727],\n [32, [143], 0.0035125293051635254],\n [38, [30, 161], 0.025432063142193605],\n [42, [0, 13, 108], 0.040278191372408144],\n [43, [2, 11], 0.05943673183793561],\n [59, [29], 0.025432063142193605],\n [83, [266], 0.007266303754912459],\n [157, [0, 6], 0.1889238976277239],\n [169, [0, 188, 295, 302], 0.025867330726534327],\n [174, [16, 49], 0.05135795275316766],\n [202, [31, 74], 0.05037970603405972],\n [250, [84], 0.013029234319153373],\n [273, [286], 0.003163797328693941],\n [279, [330, 394], 0.008686156212768916],\n [282, [0, 20, 59, 82, 94, 113, 126, 153], 0.11315228093211271],\n [284, [5, 399], 0.008138260205501954],\n [285, [19, 163, 199], 0.027173985275220563],\n [299, [0, 36, 80, 124], 0.06151010620437523],\n [307, [10, 57], 0.038332385025914996],\n [317, [2, 12, 84], 0.08096738469759596],\n [322, [708], 0.00346649353445365],\n [325, [94, 184], 0.026852127581605427],\n [328, [125, 237, 299, 312, 346], 0.033565159477006785],\n [285, [19, 163, 199], 0.027173985275220563],\n [331, [195, 206], 0.01674009219486161],\n [332, [319], 0.0070157415564672],\n [299, [0, 36, 80, 124], 0.06151010620437523],\n [359, [0, 35, 79], 0.06781883504584961],\n [370, [31, 144, 180], 0.035904089141920385],\n [382, [145, 275, 480], 0.009820301609361886],\n [384, [353], 0.005688031326426096],\n [431, [70, 105], 0.012188638556627348],\n [434, [73], 0.01973831766259802],\n [479, [108], 0.014453194354033524],\n [490, [0, 6], 0.22999431015548996],\n [493, [83], 0.0293881618532015],\n [569, [1092, 1153, 1254, 1310, 1374], 0.00942599631784795],\n [599, [248], 0.009057995091740188],\n [602, [59], 0.028440156632130485],\n [603, [164], 0.007688763275546903],\n [608, [156], 0.013224672833940673],\n [612, [169], 0.01235950732143988],\n [640, [52, 165, 317, 337], 0.030844717980036556],\n [681, [517], 0.004335958306210057],\n [684, [2, 13], 0.24044859698073953],\n [686, [24, 189], 0.024377277113254697],\n [687, [22], 0.0480897193961479],\n [689, [667], 0.003130100079039213],\n [690, [36], 0.0176328971119209],\n [691, [0, 10, 32, 88, 200, 227, 289, 305], 0.05943673183793561],\n [695, [0], 0.02404485969807395],\n [696, [38], 0.03148731627128732],\n [698, [0, 9, 51, 85], 0.053704255163210854],\n [701, [310], 0.007090977390852908],\n [702, [55, 204, 224, 233, 299, 337, 360], 0.044721115863567494],\n [709, [2, 15], 0.20345650513754884],\n [713, [0, 11], 0.05749857753887249],\n [728, [6, 21, 102, 192, 227, 247, 287, 387, 405, 425], 0.048442025032749726],\n [733, [297], 0.008875619351638036],\n [745, [73], 0.018240928046814722],\n [755, [2, 13], 0.036481856093629444],\n [713, [0, 11], 0.05749857753887249],\n [761, [72, 142, 165, 284, 298], 0.02893801495391832],\n [767, [73, 274, 499, 664], 0.015332954010365999],\n [802, [6, 21, 102, 192], 0.05110984670121999],\n [803, [4, 17, 210], 0.031116877256330996],\n [701, [310], 0.007090977390852908]]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "worddic['japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "''"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# create word search which takes multiple words and finds documents that contain both along with metrics for ranking:\n",
    "\n",
    "    ## (1) Number of occruances of search words \n",
    "    ## (2) TD-IDF score for search words \n",
    "    ## (3) Percentage of search terms\n",
    "    ## (4) Word ordering score \n",
    "    ## (5) Exact match bonus \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def search(searchsentence):\n",
    "    try:\n",
    "        # split sentence into individual words \n",
    "        searchsentence = searchsentence.lower()\n",
    "        try:\n",
    "            words = searchsentence.split(' ')\n",
    "        except:\n",
    "            words = list(words)\n",
    "        enddic = {}\n",
    "        idfdic = {}\n",
    "        closedic = {}\n",
    "        \n",
    "        # remove words if not in worddic \n",
    "        realwords = []\n",
    "        for word in words:\n",
    "            if word in list(worddic.keys()):\n",
    "                realwords.append(word)  \n",
    "        words = realwords\n",
    "        numwords = len(words)\n",
    "        \n",
    "        # make metric of number of occurances of all words in each doc & largest total IDF \n",
    "        for word in words:\n",
    "            for indpos in worddic[word]:\n",
    "                index = indpos[0]\n",
    "                amount = len(indpos[1])\n",
    "                idfscore = indpos[2]\n",
    "                #enddic[index] = amount\n",
    "                try:\n",
    "                    enddic[index].append(amount)\n",
    "                except:\n",
    "                    enddic[index] = []\n",
    "                    enddic[index].append(amount)\n",
    "\n",
    "                idfdic[index] = idfscore\n",
    "                fullcount_order = sorted(enddic.items(), key=lambda x:np.sum(x[1]), reverse=True)\n",
    "                fullidf_order = sorted(idfdic.items(), key=lambda x:np.sum(x[1]), reverse=True)\n",
    "            #print(fullcount_order)\n",
    "        # print(enddic)\n",
    "        #print(fullcount_order)    \n",
    "\n",
    "\n",
    "        #make metric of what percentage of words appear in each doc\n",
    "        combo = []\n",
    "        alloptions = {k: worddic.get(k, None) for k in (words)}\n",
    "        #print(alloptions.values)\n",
    "\n",
    "        for worddex in list(alloptions.values()):\n",
    "            for indexpos in worddex:\n",
    "                for indexz in indexpos:\n",
    "                    combo.append(indexz)\n",
    "        #print()\n",
    "        comboindex = combo[::3]\n",
    "        combocount = Counter(comboindex)\n",
    "        \n",
    "        for key in combocount:\n",
    "            combocount[key] = combocount[key] / numwords\n",
    "        combocount_order = sorted(combocount.items(), key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "        # make metric for if words appear in same order as in search\n",
    "        if len(words) > 1:\n",
    "            x = []\n",
    "            y = []\n",
    "            for record in [worddic[z] for z in words]:\n",
    "                for index in record:\n",
    "                     x.append(index[0])\n",
    "            print(x)\n",
    "        #     for i in x:\n",
    "        #         if x.count(i) > 1:\n",
    "        #             y.append(i)\n",
    "        #     y = list(set(y))\n",
    "\n",
    "        #     closedic = {}\n",
    "        #     for wordbig in [worddic[x] for x in words]:\n",
    "        #         for record in wordbig:\n",
    "        #             if record[0] in y:\n",
    "        #                 index = record[0]\n",
    "        #                 positions = record[1]\n",
    "        #                 try:\n",
    "        #                     closedic[index].append(positions)\n",
    "        #                 except:\n",
    "        #                     closedic[index] = []\n",
    "        #                     closedic[index].append(positions)\n",
    "\n",
    "        #     x = 0\n",
    "        #     fdic = {}\n",
    "        #     for index in y:\n",
    "        #         csum = []\n",
    "        #         for seqlist in closedic[index]:\n",
    "        #             while x > 0:\n",
    "        #                 secondlist = seqlist\n",
    "        #                 x = 0\n",
    "        #                 sol = [1 for i in firstlist if i + 1 in secondlist]\n",
    "        #                 csum.append(sol)\n",
    "        #                 fsum = [item for sublist in csum for item in sublist]\n",
    "        #                 fsum = sum(fsum)\n",
    "        #                 fdic[index] = fsum\n",
    "        #                 fdic_order = sorted(fdic.items(), key=lambda x:x[1], reverse=True)\n",
    "        #             while x == 0:\n",
    "        #                 firstlist = seqlist\n",
    "        #                 x = x + 1\n",
    "        # else:\n",
    "        #     fdic_order = 0\n",
    "                    \n",
    "        # # also the one above should be given a big boost if ALL found together \n",
    "           \n",
    "        \n",
    "        # #could make another metric for if they are not next to each other but still close \n",
    "        \n",
    "        \n",
    "        # return(searchsentence,words,fullcount_order,combocount_order,fullidf_order,fdic_order)\n",
    "    \n",
    "    except:\n",
    "        return(\"\")\n",
    "\n",
    "\n",
    "search('indonesia crude palm oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "yes\n"
    }
   ],
   "source": [
    "if(os.path.exists('worddic_1000')):\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38032bitd37838a0bbc64f33801bb6f569018b45",
   "display_name": "Python 3.8.0 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}